---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 



2. 
    a.
    
    ```{python}
    import pandas as pd
    import numpy as np

    # Read the file firstly
    df_2016 = pd.read_csv("D:/uchicago/24 fall/data/ps4/POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv", low_memory = False)
    df_2016.head()

    # Count the number of short-term hospitals
    short_term_hos_2016 = df_2016[(df_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2016['PRVDR_CTGRY_CD'] == 1)]
    print(len(short_term_hos_2016))
    ```

    From the result, we could see that the number of short-term hospitals from the dataset are 7245. This does not make sense.

    From the outer sources, we could see that, the total number of All U.S. Registered Hospitals are 5534, which is far lower than the result from dataset. (https://www.aha.org/system/files/2018-02/2018-aha-hospital-fast-facts.pdf)

    Morevoer, we cannot find the exact term of 'short-term' hospital, but we did find a similary term, which is community hospital, the data of community hospitals in 2016 is 3385+1882=5267 (https://www.ahadata.com/hospitaltrendwatch/hospitalorganizationaltrends), which is very close to the number for the total number of registered hospitals in U.S. While the definition of community hospital is community hospitals included all non-federal, short-term general and specialty hospitals whose facilities and services are available to the public. Therefore, since the community hospital has included the number of short-term general and specialty hospitals. The number from the dataset is not making any sense since it far more exceeds the number for the other resouces.



    b. From my perspective, the differences could be the following reasons:
    1. The discrepancy may be due to variations in how short-term is defined across datasets. POS may include more specialized facilities under the short-term designatio taht AHA excludes.
    2. The POS dataset might include affiliated or multilocation entries separately, whereas AHA may consolidate these under single institutional entries.
    3. The POS dataset might have wrong data entry progress, making the dataset includes duplicate hospitals.

3. 

```{python}
# Load the datasets
df_2017 = pd.read_csv('D:/uchicago/24 fall/data/ps4/POS_File_Hospital_Non_Hospital_Facilities_Q4_2017.csv', low_memory = False, encoding='ISO-8859-1')
df_2018 = pd.read_csv('D:/uchicago/24 fall/data/ps4/POS_File_Hospital_Non_Hospital_Facilities_Q4_2018.csv', low_memory = False, encoding='ISO-8859-1')
df_2019 = pd.read_csv('D:/uchicago/24 fall/data/ps4/POS_File_Hospital_Non_Hospital_Facilities_Q4_2019.csv', low_memory = False, encoding='ISO-8859-1')

# Calculate the quantities for each year
short_term_hos_2017 = df_2017[(df_2017['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2017['PRVDR_CTGRY_CD'] == 1)]
short_term_hos_2018 = df_2018[(df_2018['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2018['PRVDR_CTGRY_CD'] == 1)]
short_term_hos_2019 = df_2019[(df_2019['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2019['PRVDR_CTGRY_CD'] == 1)]

# Quantities
print(len(short_term_hos_2017))
print(len(short_term_hos_2018))
print(len(short_term_hos_2019))

# presetting
short_term_hos_2016_selected = short_term_hos_2016[['PRVDR_CTGRY_SBTYP_CD', 'PRVDR_CTGRY_CD']]
short_term_hos_2017_selected = short_term_hos_2016[['PRVDR_CTGRY_SBTYP_CD', 'PRVDR_CTGRY_CD']]
short_term_hos_2018_selected = short_term_hos_2016[['PRVDR_CTGRY_SBTYP_CD', 'PRVDR_CTGRY_CD']]
short_term_hos_2019_selected = short_term_hos_2016[['PRVDR_CTGRY_SBTYP_CD', 'PRVDR_CTGRY_CD']]
short_term_hos_2016_selected['Year'] = 2016
short_term_hos_2017_selected['Year'] = 2017
short_term_hos_2018_selected['Year'] = 2018
short_term_hos_2019_selected['Year'] = 2019

# Append them together
total_short = pd.concat([short_term_hos_2016_selected, short_term_hos_2017_selected, short_term_hos_2018_selected, short_term_hos_2019_selected], ignore_index = True)

# Plot the quantities over time
yearly_counts = total_short.groupby('Year').size()
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 8))
yearly_counts.plot(kind = 'bar', color = 'skyblue', edgecolor = 'black')
plt.title("Number of Short-Term Hospitals Reported Over Year")
plt.xlabel("Year")
plt.ylabel('Number of Hospital')
plt.grid(axis ='y', linestyle = '--', alpha = 0.7)
plt.show()


```

4. 
    a.
    
    ```{python}
    # Define the function to filter unique hospitals with valid CMS Certification Numbers
    def filter_cms_hospitals(df):
        df['PRVDR_NUM'] = df['PRVDR_NUM'].astype(str)
        short_term = df[
          (df['PRVDR_NUM'].str.len().isin([6, 10])) &
          (df['PRVDR_CTGRY_SBTYP_CD'] == 1) &
          (df['PRVDR_CTGRY_CD'] == 1)
        ]
        return short_term

    # Apply the filter to each year’s data
    df_2016_4 = filter_cms_hospitals(df_2016)
    df_2017_4 = filter_cms_hospitals(df_2017)
    df_2018_4 = filter_cms_hospitals(df_2018)
    df_2019_4 = filter_cms_hospitals(df_2019)

    # Check if data is present
    print("2016:", df_2016_4.shape)
    print("2017:", df_2017_4.shape)
    print("2018:", df_2018_4.shape)
    print("2019:", df_2019_4.shape)

    # If data is present, proceed
    if all([not df.empty for df in [df_2016_4, df_2017_4, df_2018_4, df_2019_4]]):
        # Add Year column to each filtered DataFrame
        df_2016_4['Year'] = 2016
        df_2017_4['Year'] = 2017
        df_2018_4['Year'] = 2018
        df_2019_4['Year'] = 2019

        # Concatenate the DataFrames
        total_cms = pd.concat([df_2016_4, df_2017_4, df_2018_4, df_2019_4], ignore_index=True)

        # Group by Year and count unique CMS Certification Numbers
        unique_hospital_counts = total_cms.groupby('Year')['PRVDR_NUM'].nunique()

        # Plot the number of unique hospitals by CMS Certification Number
        plt.figure(figsize=(10, 8))
        unique_hospital_counts.plot(kind='bar', color='skyblue', edgecolor='red')
        plt.title('Number of Unique Hospitals by CMS Certification Number Over Year')
        plt.xlabel('Year')
        plt.ylabel('Number of Unique Hospitals')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.show()
    else:
        print("No data found for one or more years after filtering.")
    ```


    b. According to the output, we can see that these two plots have no different, for years of 2016, 2017, 2018 and 2019, they all have the same quantities of unique hospitals with the quantities of hospital we have calculated in the last question.

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
# Since we have defined the df_2016 and df_2019 before
# Filter the active hospital in 2016
active_2016 = df_2016[
    (df_2016['PGM_TRMNTN_CD'] == 0) &
    (df_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) &
    (df_2016['PRVDR_CTGRY_CD'] == 1)
][['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].copy()

active_2016['Year_Active'] = 2016

# Combine 2017-2019 data and add Year column for each dataset
def prepare_year_data(df, year):
    """
    Filters the dataframe for short-term hospitals and adds a Year column.
    """
    df_filtered = df[
        (df['PRVDR_CTGRY_SBTYP_CD'] == 1) &
        (df['PRVDR_CTGRY_CD'] == 1)
    ].copy()
    df_filtered['Year'] = year
    return df_filtered[['PRVDR_NUM', 'PGM_TRMNTN_CD', 'TRMNTN_EXPRTN_DT', 'Year']]

df_2017_21 = prepare_year_data(df_2017, 2017)
df_2018_21 = prepare_year_data(df_2018, 2018)
df_2019_21 = prepare_year_data(df_2019, 2019)

# Concatenate data from 2017 to 2019
all_years = pd.concat([df_2017_21, df_2018_21, df_2019_21], ignore_index=True)


# Extract year from TRMNTN_EXPRTN_DT where available
all_years['Closure_Year'] = pd.to_datetime(all_years['TRMNTN_EXPRTN_DT'], errors='coerce').dt.year
all_years['Closure_Year'] = all_years['Closure_Year'].fillna(all_years['Year'])

all_years_agg = all_years.groupby(['PRVDR_NUM', 'Year']).agg({
    'PGM_TRMNTN_CD': 'max',
    'Closure_Year': 'max'
}).reset_index()

# define the first year of closure
closures = []

for index, hospital in active_2016.iterrows():
    prvd_num = hospital['PRVDR_NUM']
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    
    hospital_records = all_years_agg[all_years_agg['PRVDR_NUM'] == prvd_num]
    
    closure_year = None
    
    for year in range(2017, 2020):
        record = hospital_records[hospital_records['Year'] == year]
        
        if record.empty:
            closure_year = year
            break
        elif record['PGM_TRMNTN_CD'].iloc[0] != 0:
            closure_year = int(record['Closure_Year'].iloc[0])
            break
    
    if closure_year:
        closures.append({
            'Facility_Name': facility_name,
            'ZIP_Code': zip_code,
            'Year_of_Closure': closure_year,
            'PRVDR_NUM':prvd_num
        })


df_closures = pd.DataFrame(closures)
quantity_closures = df_closures.shape[0]

# display the total amount and list
print(f"There will be {quantity_closures} of suspected hospital closures satisfy the definition")
```



2. 

```{python}
# Sort by list
df_closures_sorted = df_closures.sort_values(by='Facility_Name')[['Facility_Name', 'Year_of_Closure']].head(10)
print(df_closures_sorted)
```

3. 
    a.
    
    ```{python}
    # Get active hospital for each year
    def get_active_hospitals(df, year):
      return df[
        (df['PGM_TRMNTN_CD'] == 0) &
        (df['PRVDR_CTGRY_SBTYP_CD'] == 1) &
        (df['PRVDR_CTGRY_CD'] == 1)
      ][['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].copy()

    
    active_2016 = get_active_hospitals(df_2016, 2016)
    active_2017 = get_active_hospitals(df_2017, 2017)
    active_2018 = get_active_hospitals(df_2018, 2018)
    active_2019 = get_active_hospitals(df_2019, 2019)

    active_2016['Year'] = 2016
    active_2017['Year'] = 2017
    active_2018['Year'] = 2018
    active_2019['Year'] = 2019

    active_all_years = pd.concat([active_2016, active_2017, active_2018, active_2019], ignore_index=True)

    # Calculate the number of active hospitals per ZIP code per year
    active_counts = active_all_years.groupby(['ZIP_CD', 'Year']).agg(
        Active_Hospitals=('PRVDR_NUM', 'nunique')
    ).reset_index()


    # Exclude the clousres in 2019
    df_closures_excl_2019 = df_closures[df_closures['Year_of_Closure'] < 2019].copy()

    # Merge closures with active_counts to get active hospital counts for closure year
    closures_with_counts = df_closures_excl_2019.merge(
        active_counts,
        left_on=['ZIP_Code', 'Year_of_Closure'],
        right_on=['ZIP_CD', 'Year'],
        how='left'
    ).rename(columns={'Active_Hospitals': 'Closure_Year_Active'})


    # Shift the 'Year' to 'Year_of_Closure + 1' to get next year's count
    closures_with_counts['Next_Year'] = closures_with_counts['Year_of_Closure'] + 1

    closures_with_counts = closures_with_counts.merge(
        active_counts,
        left_on=['ZIP_Code', 'Next_Year'],
        right_on=['ZIP_CD', 'Year'],
        how='left'
    ).rename(columns={'Active_Hospitals': 'Next_Year_Active'})

    # Determine potential merger/acquisition
    closures_with_counts['Potential_Merger_Acq'] = closures_with_counts['Next_Year_Active'] >= closures_with_counts['Closure_Year_Active']

    # Number of potential mergers/acquisitions
    num_potential_mergers = closures_with_counts['Potential_Merger_Acq'].sum()

    # Create a list of PRVDR_NUMs that are potential mergers/acquisitions
    potential_mergers_prvdr = closures_with_counts[closures_with_counts['Potential_Merger_Acq']]['PRVDR_NUM']

    # Number of closures before removal
    total_suspected_closures = df_closures.shape[0]

    # Number of closures after removing potential mergers/acquisitions
    df_closures_corrected = df_closures[~df_closures['PRVDR_NUM'].isin(potential_mergers_prvdr)].copy()

    # Number of closures after correction
    num_corrected_closures = df_closures_corrected.shape[0]

    # Reporting the numbers
    print(f"Number of suspected closures potentially due to mergers/acquisitions: {num_potential_mergers}")

    ```

    b.

    ```{python}
    # Display
    print(f"Number of actual suspected closures after correction: {num_corrected_closures}")
    ```

    c.

    ```{python}
    df_closures_sorted_corrected = df_closures_corrected.sort_values(by='Facility_Name')[['Facility_Name', 'Year_of_Closure']].head(10)
    print("\nFirst 10 corrected hospital closures sorted by name:")
    print(df_closures_sorted_corrected)
    ```



## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
